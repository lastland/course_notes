\documentclass{beamer} % "Beamer" is a word used in Germany to mean video projector. 

\usetheme{Penn} % Search online for beamer themes to find your favorite or use the Berkeley theme as in this file.


\usepackage{multicol}
\usepackage{color} % It may be necessary to set PCTeX or whatever program you are using to output a .pdf instead of a .dvi file in order to see color on your screen.
\usepackage{graphicx} % This package is needed if you wish to include external image files.
\usepackage{collcell}
\theoremstyle{definition} % See Lesson Three of the LaTeX Manual for more on this kind of "proclamation."
\newtheorem*{dfn}{A Reasonable Definition}     

\def \pausenl {\pause $ \ $\\}          

\title{Riffled Independence for Ranked Data}
\subtitle{Jonathan Huang and Carlos Guestrin \\ NIPS, 2009}
\author{Zachary Schutzman} 
\institute{University of Pennsylvania}
%\date{January 6, 2012} 
% Remove the % from the previous line and change the date if you want a particular date to be displayed; otherwise, today's date is displayed by default.

\AtBeginSection[]  % The commands within the following {} will be executed at the start of each section.
{
\begin{frame} % Within each "frame" there will be one or more "slides."  
\frametitle{Presentation Outline} % This is the title of the outline.
\tableofcontents[currentsection]  % This will display the table of contents and highlight the current section.
\end{frame}
} % Do not include the preceding set of commands if you prefer not to have a recurring outline displayed during your presentation.

\begin{document}

\begin{frame} 
\titlepage
\end{frame}

\section{Introduction}
\begin{frame}
	\frametitle{Introduction}
\pause
Distributions over permutations are important!
\pause
\begin{itemize}
	\item Object tracking \pause
	\item \textbf{Ranked data}
	\end{itemize}
	
	\pausenl
	But, there are computational challenges
	\begin{itemize}
		\item Storage complexity \pause
		\item Time complexity
		\end{itemize}
		
		\pausenl
		
	Can we exploit properties of the distributions to reduce these difficulties?

\end{frame}
\begin{frame}
	\frametitle{Where we're going:}
	\begin{itemize}
		\item Probabilistic vs Riffle Independence
		\pause
		\item Riffle Distributions \pause
		\item Fourier Domain Algorithms
	\end{itemize}
\end{frame}

\section{Some Math}

\begin{frame}
	\frametitle{The Symmetric Group}
	\pause
	The set of permutations on $n$ objects forms the group $S_n$\pause
	\begin{itemize}
		\item The composition of permutations is a permutation\pause
		\item Every permutation has a (unique) inverse permutation
	\end{itemize}
	\pause
	$|S_n| = n!$
\end{frame}

\begin{frame}
	\frametitle{Composing permutations}
	This is a very straightforward process, but it's super important!\\
	\pausenl
	
	Let's take two permutations in $S_5$, $\sigma=[[34125]]$ and $\tau = [[54231]]$\\
	\pausenl
	Writing `$\sigma \tau$' means `first do $\tau$, then do $\sigma$', like $g(f(x))$.\\
	
	\pausenl
	
	Computing $\sigma\tau$ is really easy:\\
	\pause
	\begin{itemize}
		\item $1\xrightarrow{\tau} 5 \xrightarrow{\sigma} 5$ \pause $\Rightarrow [[xxxx1]]$
		\pause
		\item $2\xrightarrow{\tau} 3 \xrightarrow{\sigma} 1$ \pause $\Rightarrow [[2xxx1]]$
		\pause
		\item $3\xrightarrow{\tau} 4 \xrightarrow{\sigma} 2$ \pause $\Rightarrow [[23xx1]]$
		\pause
		\item $4 \xrightarrow{\tau} 2 \xrightarrow{\sigma} 4$ \pause $\Rightarrow [[23x41]]$
		\pause
		\item $5 \xrightarrow{\tau} 1 \xrightarrow{\sigma} 3$ \pause $\Rightarrow [[23541]]$
	\end{itemize}
	
	
\end{frame}

\begin{frame}
	\frametitle{Representations of the Symmetric Group}
	\pause
	Group elements aren't easy to use in computations\\
	\pausenl
	But matrices (vector space elements) are!\\
	
	\pausenl
	
	A \textbf{group representation} is a map $\rho: G \rightarrow GL(k)$ which preserves the group structure\\
	$\rho(g)\rho(h)=\rho(gh)$\\
	\pausenl
	
	We can always make a new representation by taking the direct sum of two other representations.\\
	\pause
	An \textbf{irreducible representation} is one that cannot be written as the direct sum of two others.
\end{frame}


\begin{frame}
	\frametitle{Example: Representations of $S_3$}
	
		\begin{center}
			\begin{tabular}{c<{\onslide<3->}c<{\onslide<4->}c<{\onslide<5->}c<{\onslide<6->}c<{\onslide<2->}}
				$\pi$ & $g$ &           $\rho_{triv}$ 	& $\rho_{sgn}$ 		& $\rho_{std}$ 			\\
				$123$   & $()$   & $1$ 			& $1$      			& $\begin{pmatrix} 1 & 0  \\ 0 & 1 \end{pmatrix} $ \\
				$213$   & $(12)$   	& $1$ 			& $-1$      		& $\begin{pmatrix} -1 & 1  \\ 0 & 1 \end{pmatrix} $     		\\
				$132$   & $(23)$   	& $1$ 			& $-1$      		& $\begin{pmatrix} 1 & 0  \\ 1 & -1 \end{pmatrix} $    												\\
				$321$   & $(13)$   		& $1$ 			& $-1$      		& $\begin{pmatrix} 0 & -1  \\ -1 & 0 \end{pmatrix} $     \\			
				$231$   & $(132)$   	& $1$ 			& $1$      			&$\begin{pmatrix} -1 & 1  \\ -1 & 0 \end{pmatrix} $     		\\
				$312$   & $(123)$   	& $1$ 			& $1$      			& $\begin{pmatrix} 0 & -1  \\ 1 & -1 \end{pmatrix} $     		
				
			\end{tabular}
			
		\end{center}	
		
		\pause[8]
		
		Are there any other irreducible representations?
		

\end{frame}

\begin{frame}
	\frametitle{The Fourier Transform}
	
	Given a function $f:G\rightarrow \mathbb{R}$, the Fourier transform at a representation $\rho$ is\\
	\pause
	$\hat{f}_\rho = \sum\limits_{g\in G} f(g)\rho(g)$\\
	
	\pausenl
	
	$\hat{f}_\rho$ is a matrix of the same dimension as $\rho$.\\\pausenl
	
	The family of $\hat{f}_\rho$ corresponding to the irreducible representations of $G$ systematically encode $f$.
\end{frame}


\section{Distributions on Permutations and Independence}

\begin{frame}
	\frametitle{Distributions}
	
	Keeping the idea of the Fourier transform in the back of our minds...\\
	\pausenl
	A distribution on $S_n$ is a function $h: S_n \rightarrow \mathbb{R}^+$ such that $\sum\limits_{\sigma\in S_n} h(\sigma) = 1$.\\
	
	\pausenl
	
	To characterize an arbitrary distribution we need $n!$ parameters.\\
	
	\pausenl
	What if we have two distinct `classes' of objects?  Denote ${X} = \{1,2,\dots,p\}$ and $\overline{X} = \{p+1,\dots,n\}$.  Let's call $q=n-p$.
	
	
	
	
\end{frame}

\begin{frame}
	\frametitle{Probabilistic Independence}
	
	Let $f$ be a distribution over $S_p$, corresponding to permutations on $X$ and $g$ the same for $\overline{X}$ over $S_q$.\\
	
	\pausenl
	
	We say that $h$ is \textbf{probabilistically independent} if we can write $h(\sigma) = f(\sigma_p)\cdot g(\sigma_q)$.\\ 
	
	\pausenl
	
	If $h$ does factor this way, why is that nice?
	\pausenl
	We can know $h$ with only $p! + q!$ parameters.\\
	
	\pausenl
	
	What is the problem with this?
	
	\pausenl
	
	It doesn't allow any expression of preferences between the two classes!
\end{frame}

\begin{frame}
	\frametitle{Interleavings}
	
	What if we allow an interleaving?\\
	$\ $\\
	
	A \textbf{(p,q)-interleaving} is a permutation in $S_n$ that preserves the relative ordering within the $p$- and $q$-subsets.  We will call the set of all such interleavings $\Omega_{p,q}$\\
	
	\pausenl
	
	For example, if $\sigma_p = [[1342]]$ and $\sigma_q = [[6875]]$, then one possible interleaving is $[[13648725]]$, corresponding to the (p,q)-interleaving $\tau = [[12536748]]$.\\
	
	\pausenl
	
	Does fixing an interleaving fix our problem?
	\pausenl
	Not really. 
	
\end{frame}

\begin{frame}
	\frametitle{Riffle Distributions}
	
	What if we allow a \textit{distribution} over the interleavings?\\
	
	\pausenl
	
	Let $m_{p,q}$  be a distribution over $\Omega_{p,q}$.  Equivalently, $m_{p,q}$ is a distribution over $S_n$ which assigns probability zero to elements not in $\Omega_{p,q}$.\\
	
	\pausenl
	
	The \textbf{uniform riffle distribution}, denoted $m_{p,q}^{unif}$ is the one which assigns probability $\frac{1}{|\Omega_{p,q}|}$ to each shuffle in $\Omega_{p,q}$.\\
	
	\pausenl
	
	The uniform riffle distribution properly expresses indifference between $X$ and $\overline{X}$ as classes, while still allowing for preferences within each class.
	
\end{frame}

\begin{frame}
	\frametitle{Riffle Independence}
	
	Define two $f$ and $g$ distributions as before to be \textbf{riffle independent} if $h$ factors as $h(\sigma) = m_{p,q} \ast (f(\sigma_p)\cdot g(\sigma_q)$.\\
	
	\pausenl
	
	Let's note that the delta distribution on any one shuffle (i.e. fixing a shuffle) gets us back to the probabilistic independence case.  We can see that riffle independence is a generalization of probabilistic independence.\\
	
	\pausenl
	
	We need an additional $\binom{n}{p}$ parameters to characterize $m_{p,q}$, so storing a riffle independent distribution requires $\binom{n}{p} + p! + q!$ parameters.
	
	
\end{frame}


\begin{frame}
	\frametitle{Biased Riffle Shuffles}
	
	The authors describe a family of riffle distributions called \textbf{biased riffle distributions}.\\
	\pause
	These allow for expressing some weighted preference for one group over the other.\\
	
	\pausenl
	
	For a bias parameter $\alpha$, we can recursively construct a draw from $m_{p,q}^\alpha$ by appending to our final permutation the top item remaining in $\sigma_p$ with probability proportional to $\alpha$ and the fraction of items remaining in $\sigma_p$.\\
	\pausenl
	
	Note that $\alpha=.5$ gives us $m_{p,q}^{unif}$, $\alpha=0$ says we prefer everything in $\overline{X}$ over anything in $X$, and $\alpha=1$ is the opposite.
\end{frame}
\begin{frame}
	
\frametitle{Conditional Independence}

We might be inclined to wonder how riffle independence differs from conditional independence.\\

\pausenl

If instead we first draw a permutation of the ranks for the $p$ items, then draw a $\sigma_p$ and $\sigma_q$, we get something that feels like $f$ and $g$ being conditionally independent given the chosen ranks.\\

\pausenl

But conditional independence requires $\binom{n}{p} (p!+q! + 1)$ parameters, which is a further generalization of riffle independence. \\\pausenl
 We can therefore say riffle independence lives somewhere between probabilistic and conditional independence.
\end{frame}

\section{Fourier Domain Algorithms}

\begin{frame}
	\frametitle{Fourier Transform on Distribution}
	
	Let's observe that the Fourier transform of a distribution at the irreducible representations contains exactly enough information to fully reconstruct $h$.\\
	\pausenl
	
	It can be shown that, given an induced ordering on the representations, the Fourier matrices \textit{systematically} encode the distribution.\\
	\pausenl
	
	Define a \textbf{$k$th order marginal} as the probability of observing a particular $k$-tuple at a certain position in a permutation.\\
	
	\pausenl
	
	The first two representations contain enough information to recover the first order marginals, the next three the second order marginals, and so on.
\end{frame}

\begin{frame}
	\frametitle{RiffleJoin and RiffleSplit}
	
	The authors present two algorithms which operate in the Fourier domain.\\\pausenl
	
	RiffleJoin takes the Fourier matrices for $f$, $g$, and $m$ and computes the Fourier matrices for $h$.\\
	
	RiffleSplit takes the Fourier matrix for $h$, factors out the uniform shuffle's dual's Fourier matrix, then splits $f$ and $g$.\\\pausenl
	
	It can be shown that if $f$ and $g$ are not perfectly riffle independent, RiffleSplit returns estimates for factors which are as independent as possible.
\end{frame}


\begin{frame}
	\frametitle{Some experiments}
	
	The authors tested their algorithm on two data sets: the American Psychological Association election and the Sushi Data Set.
\end{frame}

\begin{frame}
	\frametitle{Experiment 1: APA Election}
	
	The data:  5738 ballots of ranked preferences across 5 candidates for president of the APA\\
	\pause
	The hypothesis: {C1,C3} and {C4,C5} fall on opposite ends of a political spectrum.  Voters may express a preference for one group over the other, then express preferences within the group.\\\pause
	The experiment: ignoring C2, decompose the full distribution into riffle factors.\\
	\pausenl
	
	The results:  The authors find that {C1,C3} and {C4,C5} are nearly riffle independent, and that fitting a mixture-of-riffles model yields bias parameters that strongly suggest their hypothesis is correct. 
	
\end{frame}

\begin{frame}
	\frametitle{Experiement 2: Sushi}
	The data: 5000 full rankings of 10 different kinds of sushi (note, WAY fewer than 10! observations).  The authors divide this into two groups of five. \\\pause
	The hypothesis: the full distribution can be approximated by combining an optimal $m$ with two factor subsets and/or by finding an optimal bias parameter $\alpha$.\\\pause
	
	The experiment: Try to learn the true distribution using these two methods\\
	\pausenl
	
	The results:  Assuming riffle independence significantly lowers the sample complexity required to learn the distribution.\\
	\pause Biased riffle shuffles are a useful tool for learning on small samples.
\end{frame}

\section{Questions and discussion}
\begin{frame}
	\frametitle{Discussion}
	What are some problems where riffle independence might lend some new insight?\\$\ $\\
	
	What if the riffle factors are non-obvious?\\$\ $\\
	
	How about if there are 3+ subsets we want to study?\\$\ $\\
	
	Anything else?
\end{frame}
\end{document}

