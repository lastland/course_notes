%
% This is the LaTeX template file for lecture notes for CS294-8,
% Computational Biology for Computer Scientists.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi.
%
% This template is based on the template for Prof. Sinclair's CS 270.

\documentclass[twoside]{article}
\usepackage{graphics}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{IEEEtrantools}
\usepackage{amsmath,amssymb,trimclip,adjustbox}

\mathchardef\mhyphen="2D % Define a "math hyphen"

\usetikzlibrary{chains,fit,shapes}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\chno}[4]{
   \pagestyle{headings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf CIS 511: Theory of Computation
                        \hfill Mar 28, 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Professor #3 \hfill #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
   {\bf NB}: {\it These notes are from CIS511 at Penn. The course followed Michael Sipser's \textit{Introduction to the Theory of Computation (3ed)} text.}
   \vspace*{4mm}
}

%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\chno{18}{Randomized Complexity}{Sampath Kannan}{Zach Schutzman}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.



\section*{Randomized Algorithms}

\definition{A \textbf{randomized algorithm} is one in which we can toss random coins.}

We don't worry about \textit{how} this randomness is produced, we just take it as given that we can make these queries from an idealized source of randomness.

\definition{A \textbf{randomized} or \textbf{probabilistic Turing machine} may either move deterministically or reach a state where it can flip a coin and move according to the random output.  A probabilistic Turing machine $M$ is said to accept a string $x\in L$ with probability equal to the sum of the probabilities of the leaves where $M$ produces the correct answer.}

\definition{The class \textbf{Bounded Probabilistic Polynomial time (BPP)} is the class of languages recognizable by a randomized Turing machine which accept on inputs $x\in L$ with probability at least $\frac{2}{3}$.}

\claim{We can equivalently define $BPP$ as having error $0\leq \epsilon < \frac{1}{2}$.}

\begin{proof}
	Suppose we have a machine $M$ with error $\epsilon < \frac{1}{2}$.  Then we show there is an $M'$ with error $\frac{1}{2^n}$ on inputs of size $n$.  We simply have $M'$ run $M$ $2k$ times on input $x$ and outputs the majority answer, where $k$ is constructed as follows:
	
	To see this, consider our sequence of $M$'s outputs, e.g. $cccwwcwccwc\dots$.  A sequence with more than $k$ $w$s will cause an error in $M'$.  We want this to occur with probability at most $\frac{1}{2^n}$.  Any particular sequence with some number of $c$s and $w$s is $\epsilon^{\#w}(1-\epsilon)^{\#c}$.  So $M'$ makes a mistake when $\#w\geq k$.  But this is less than $\epsilon^k (1-\epsilon)^{k}$.  There are at most $2^{2k}$ `bad' sequences (this is all of them!).  Thus, the total probability of all bad sequences is less than or equal to $2^{2k}\epsilon^k (1-\epsilon)^k = (4\epsilon(1-\epsilon))^k$.  This is strictly less than one whenever $\epsilon<\frac{1}{2}$.
	
	We can then choose $k$ to make the lower bound as low as we want for an equivalent definition of $BPP$.
\end{proof}

What is the space complexity of probabilistic Turing machines?

Consider the language $UPATH = \{ \langle G,s,t \rangle | \ \ there \ is \ an \ st \ path \ in \ G     \}$.  This is in randomized log-space ($RL$).  We can do this by randomly choosing a neighbor at each step, not exceeding $n^3$ steps on any branch.




\subsection*{Polynomial Identity}
The problem of polynomial identity testing asks whether two multivariate polynomials $f$ and $g$ on $n$ variables are exactly identical.  We have two black boxes, one for $f$ and one for $g$.  We want to know if $f=g$.

Denote $L_{non-id}=\{ \langle f, g \rangle | f\neq g   \}$ for polynomials $f$ and $g$.  If $f$ and $g$ are univariate of degree less than or equal to $d$, we can know if $f=g$ just by testing $d+1$ different values.  

\lemma{(Schwartz-Zippel)  Let $h$ be a non-zero polynomial on $n$ variables of degree $d$.  If values $x_1\dots x_n$ are chosen uniformly and independently at random from a set $S$ of size $n$, then the probability that $h(x)=0$ is less than or equal to $\frac{d}{n}$.}



If we think of $h=f-g$, then we have the `bad' event of getting unlucky of picking $d+1$ points where $f(x)=g(x)$ even when $f\neq g$ is sufficiently small.

Now we can come up with a randomized algorithm to recognize $L_{not-id}$.  We choose a set $S$ of $3d$ points from the domain of the variables.  For each $x_i$ pick uniformly at random some $r_i\in S$.  Now, evaluate $f(r)$ and $g(r)$.  If we get different answers, accept.  Otherwise, reject.  If $f$ and $g$ are identical, then the algorithm always correctly rejects.  If $f$ and $g$ are different, the probability we picked a `bad' point and make a mistake is at most $\frac{1}{3}$, because we have $\frac{d}{3d}$ probability by the lemma.


\definition{The language $L$ is in \textbf{Probabilistic Polytime (PP)}if there is a probabilistic Turing machine that gets the answer wrong with probability less than $\frac{1}{2}$.}

This class is not really interesting.  A $PP$ algorithm for $SAT$ picks a random assignment.  If it satisfies the formula, we accept.  If not, flip a coin with probability marginally less than $\frac{1}{2}$ to come up heads.  If we get heads, reject, otherwise, accept.


\end{document}

